{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemEval: 2019 BERT and SVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wild10/SemEval_2019_OffensiveLanguage/blob/master/SemEval_2019_BERT_and_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmXN-ujC7WR0",
        "colab_type": "text"
      },
      "source": [
        "primero vamos a **instalar BERT** y sus dependencias cada vez que iniciamos sesion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeE7mN0a7Qru",
        "colab_type": "code",
        "outputId": "e347c46e-645d-4b20-ebdd-41646dbf7146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Collecting regex (from pytorch-pretrained-bert)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 38.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.165)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.165 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.165)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.165->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.165->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.165->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tZ2tRxSjGwP",
        "colab_type": "text"
      },
      "source": [
        "Offensive Language Identification using BERT and SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq8fFiogksFD",
        "colab_type": "code",
        "outputId": "907c6275-615b-4326-b797-fef5afbb2889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#primero vamos a montar el google drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# montar el driver de accesso\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDivAPrbkQtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import logging\n",
        "import argparse\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer # tokenizador del bert\n",
        "from pytorch_pretrained_bert.modeling import BertForSequenceClassification \n",
        "from pytorch_pretrained_bert.optimization import BertAdam  # obtimizador\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsuaJHMOnfrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"Un solo training/test para una simple secuancia de clasificacion\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructor de un InputExample.\n",
        "        Args:\n",
        "            guid: id para el ejemplo\n",
        "            text_a: string. texto no tokenizado de la 1° secuencia.\n",
        "            text_b: (Opcional) string. txt no tokenizado de la 2da secuencia.\n",
        "            (para pares de secuencias).\n",
        "            label: (Optional) cadena. La etiqueta del ejemplo. Esto debe ser\n",
        "             especificados para entrenamientos y ejemplos, pero no para pruebas de prueba.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"simple conjunto de caracteristicas.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask # mascara del BERT\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base de datos para convertidores de datos para la clasificación de fechas.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets coleccion de `InputExample`s para el train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets coleccion de `InputExample`s para el devset.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets lista de labels para el dataset.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Lee un archivo de valores separados por tabulaciones.\"\"\"\n",
        "        with open(input_file, \"r\", encoding='utf-8') as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class Offen_1_Processor(DataProcessor):\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"ver clase base.\"\"\"\n",
        "        return self._create_trn_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"offenseval-training-v1.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"ver clase base.\"\"\"\n",
        "#        return self._create_test_examples(\n",
        "#            self._read_tsv(os.path.join(data_dir, \"testset-taska.tsv\")), \"dev\")\n",
        "        return self._create_dev_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"offenseval-trial.txt\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"ver clase base.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_trn_examples(self, lines, set_type):\n",
        "        \"\"\" creamos ejemplo para el training y dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "#            guid = \"%s-%s\" % (set_type, i)\n",
        "            guid = line[0]\n",
        "            text_a = line[1]\n",
        "            if line[2] == \"OFF\":\n",
        "                label = '1'\n",
        "            else:\n",
        "                label = '0'\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "    def _create_dev_examples(self, lines, set_type):\n",
        "        \"\"\"creamos ejemplo para el training y dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[0]\n",
        "            if line[1] == \"OFF\":\n",
        "                label = '1'\n",
        "            else:\n",
        "                label = '0'\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "    def _create_test_examples(self,lines,set_type):\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i != 0:\n",
        "    #            guid = \"%s-%s\" % (set_type, i)\n",
        "                guid = line[0]\n",
        "                text_a = line[1]\n",
        "                label = '0'\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "class Offen_2_Processor(DataProcessor):\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"ver clase base.\"\"\"\n",
        "        return self._create_trn_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"offenseval-training-v1.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"ver clase base.\"\"\"\n",
        "        return self._create_dev_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"offenseval-trial.txt\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"ver clase base.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_trn_examples(self, lines, set_type):\n",
        "        # creamos...\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if line[3] != \"subtask_b\" and line[3] != \"NULL\":\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = line[1]\n",
        "                if line[3] == \"TIN\":\n",
        "                    label = '1'\n",
        "                elif line[3] == \"UNT\":\n",
        "                    label = '0'\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "\n",
        "        return examples\n",
        "\n",
        "    def _create_dev_examples(self, lines, set_type):\n",
        "        # creamos...\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if line[2] != \"subtask_b\" and line[2] != \"NULL\":\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = line[0]\n",
        "                if line[2] == \"TIN\":\n",
        "                    label = '1'\n",
        "                elif line[2] == \"UNT\":\n",
        "                    label = '0'\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "\n",
        "        return examples\n",
        "\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\" cargar data en una lista de `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids: 0   0   0   0  0     0 0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        label_id = label_map[example.label]\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids,\n",
        "                              label_id=label_id))\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"truncamos un par de secuencias a la longitud maxima\"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    Esta es una heurística simple que siempre truncará la secuencia\n",
        "    más larga un token a la vez. Esto tiene más sentido que truncar un\n",
        "    porcentaje igual de tokens de cada una, ya que si una secuencia es\n",
        "    muy corta, es probable que cada token truncado contenga más información\n",
        "    que una secuencia más larga.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "def accuracy(out, labels):\n",
        "    outputs = np.argmax(out, axis=1)\n",
        "    return np.sum(outputs == labels)\n",
        "\n",
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPD6BFWKdwva",
        "colab_type": "text"
      },
      "source": [
        "instalar apex en python "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3juV0xUd3QX",
        "colab_type": "code",
        "outputId": "3f460839-27e9-406d-dabe-7682ff87e5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 4693 (delta 26), reused 35 (delta 18), pack-reused 4636\u001b[K\n",
            "Receiving objects: 100% (4693/4693), 8.73 MiB | 20.00 MiB/s, done.\n",
            "Resolving deltas: 100% (3038/3038), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52nRQloReB8Y",
        "colab_type": "code",
        "outputId": "67691123-37e5-4d5f-d0be-c9780cfefd0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4498
        }
      },
      "source": [
        "!cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:244: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-jtyioubu\n",
            "Created temporary directory: /tmp/pip-req-tracker-kubwz3ux\n",
            "Created requirements tracker '/tmp/pip-req-tracker-kubwz3ux'\n",
            "Created temporary directory: /tmp/pip-install-rcpjgniv\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-hdnvttpw\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-kubwz3ux'\n",
            "    Running setup.py (path:/tmp/pip-req-build-hdnvttpw/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "    Warning: Torch did not find available GPUs on this system.\n",
            "     If your intention is to cross-compile, this is not an error.\n",
            "\n",
            "    torch.__version__  =  1.1.0\n",
            "    running egg_info\n",
            "    creating pip-egg-info/apex.egg-info\n",
            "    writing pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-hdnvttpw has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-kubwz3ux'\n",
            "Skipping bdist_wheel for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-du_02gcq\n",
            "    Running command /usr/bin/python3 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-req-build-hdnvttpw/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-du_02gcq/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "    Warning: Torch did not find available GPUs on this system.\n",
            "     If your intention is to cross-compile, this is not an error.\n",
            "\n",
            "    torch.__version__  =  1.1.0\n",
            "    No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "    Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "    Cuda compilation tools, release 10.0, V10.0.130\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    running build_ext\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_adam_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-du_02gcq/install-record.txt'\n",
            "  Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-hdnvttpw\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-kubwz3ux'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3AgYXnnjhvX",
        "colab_type": "code",
        "outputId": "1da5516a-4ff3-48d6-b74e-25e14b6ee45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Ex3dkX7YpD",
        "colab_type": "text"
      },
      "source": [
        "Main llamada principal a las funciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPXmxqu8Gted",
        "colab_type": "code",
        "outputId": "a1719e0b-0445-47f9-e0ca-9fcf9e899237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17387
        }
      },
      "source": [
        "def main(): \n",
        "  # parametros principales\n",
        "  data_dir = \"/gdrive/My Drive/SemEval2019/training\"\n",
        "  bert_model = \"bert-base-uncased\"\n",
        "  task_name = \"one\"\n",
        "  output_dir = \"/gdrive/My Drive/SemEval2019/output/\"\n",
        "  # otroa parametros\n",
        "  max_seq_length = 80\n",
        "  do_train = True\n",
        "  do_eval = True\n",
        "  do_lower_case = True\n",
        "  train_batch_size = 32\n",
        "  learning_rate = 5e-5\n",
        "  num_train_epochs = 2\n",
        "  print(\"task_name: \",task_name)\n",
        "  print(\"do_train: \", do_train)\n",
        "  print(\"do_eval: \", do_eval)\n",
        "  print(\"do_lower_case: \", do_lower_case)\n",
        "  print(\"data_dir: \", data_dir)\n",
        "  print(\"bert_model: \",bert_model)\n",
        "  print(\"max_seq_length: \", max_seq_length)\n",
        "  print(\"train_batch_size: \", train_batch_size)\n",
        "  print(\"learning_rate: \", learning_rate)\n",
        "  print(\"num_train_epochs: \", num_train_epochs)\n",
        "  print(\"output_dir: \", output_dir)\n",
        "\n",
        "  processors = {\n",
        "        \"one\": Offen_1_Processor,\n",
        "        \"two\": Offen_2_Processor\n",
        "    }\n",
        "\n",
        "  num_labels_task = {\n",
        "        \"one\": 2,\n",
        "        \"two\": 2\n",
        "  }\n",
        "  # parametros adicionales defaul\n",
        "  local_rank = -1\n",
        "  no_cuda = True\n",
        "  \n",
        "  fp16 = False #True\n",
        "  gradient_accumulation_steps = 1\n",
        "\n",
        "  '''\n",
        "  if local_rank == -1 or no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "  else:\n",
        "  \ttorch.cuda.set_device(local_rank)\n",
        "    device = torch.device(\"cuda\",local_rank)\n",
        "    n_gpu = 1\n",
        "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "   '''\n",
        "  if local_rank == -1 or no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
        "        n_gpu = torch.cuda.device_count()\n",
        "  else:\n",
        "      torch.cuda.set_device(local_rank)\n",
        "      device = torch.device(\"cuda\", local_rank)\n",
        "      n_gpu = 1\n",
        "      # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "      torch.distributed.init_process_group(backend='nccl')\n",
        "    \n",
        "  logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
        "        device, n_gpu, bool(local_rank != -1), fp16))\n",
        "\n",
        "  if gradient_accumulation_steps < 1:\n",
        "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
        "                            gradient_accumulation_steps))\n",
        "  # gradient_accumulation_steps = 1\n",
        "  train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
        "\n",
        "  #aqui empieza la configuracion real\n",
        "  seed = 42\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if n_gpu > 0:\n",
        "  \ttorch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  if not do_train and not do_eval:\n",
        "  \traise ValueError(\"por lo menos 'do_train' o 'do_eval' deben ser True\")\n",
        "  #if os.path..\n",
        "\n",
        "  # task name en minuscula\n",
        "  task_name = task_name.lower()\n",
        "  # validacion de tarea\n",
        "  if task_name not in processors:\n",
        "    raise ValueError(\"Tarea no encontrada : %s\" % (task_name))\n",
        "\n",
        "  #print(\"processor: \", processors[task_name]())\n",
        "  #print(\"num_labels: \", num_labels_task[task_name])\n",
        "  processor = processors[task_name]()\n",
        "  num_labels = num_labels_task[task_name]\n",
        "  label_list = processor.get_labels() \n",
        "  print(\"prin label_list: \", label_list)\n",
        "  \n",
        "  tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
        "\n",
        "  train_examples = None\n",
        "  num_train_steps = None\n",
        "  if do_train:\n",
        "      train_examples = processor.get_train_examples(data_dir)\n",
        "      num_train_steps = int(\n",
        "          len(train_examples) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
        "  \n",
        "  # Prepare model\n",
        "  model = BertForSequenceClassification.from_pretrained(bert_model,\n",
        "#              cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(args.local_rank),\n",
        "            num_labels = num_labels)\n",
        "  if fp16:\n",
        "      model.half()\n",
        "  model.to(device)\n",
        "  if local_rank != -1:\n",
        "      try:\n",
        "          from apex.parallel import DistributedDataParallel as DDP\n",
        "      except ImportError:\n",
        "          raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "          \n",
        "      model = DDP(model)\n",
        "  elif n_gpu > 1:\n",
        "    model = torch.nn.DataParallel(model)\n",
        "  \n",
        "  # Prepare optimizer\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "  optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "      ]\n",
        "  t_total = num_train_steps\n",
        "  # parametro extra\n",
        "  loss_scale = 0\n",
        "  warmup_proportion = 0.1\n",
        "  eval_batch_size = 8\n",
        "  \n",
        "  if local_rank != -1:\n",
        "      t_total = t_total // torch.distributed.get_world_size()\n",
        "      \n",
        "  if fp16:\n",
        "      try:\n",
        "          from apex.optimizers import FP16_Optimizer\n",
        "          from apex.optimizers import FusedAdam\n",
        "      except ImportError:\n",
        "          raise ImportError(\"Please instala apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
        "\n",
        "      optimizer = FusedAdam(optimizer_grouped_parameters,\n",
        "                            lr=learning_rate,\n",
        "                            bias_correction=False,\n",
        "                            max_grad_norm=1.0)\n",
        "      if loss_scale == 0:\n",
        "          optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
        "      else:\n",
        "          optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n",
        "\n",
        "  else:\n",
        "      optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                           lr=learning_rate,\n",
        "                           warmup=warmup_proportion,\n",
        "                           t_total=t_total)\n",
        "  global_step = 0\n",
        "  nb_tr_steps = 0\n",
        "  tr_loss = 0\n",
        "  if do_train:\n",
        "      train_features = convert_examples_to_features(\n",
        "          train_examples, label_list, max_seq_length, tokenizer)\n",
        "      logger.info(\"***** Running training *****\")\n",
        "      logger.info(\"  Num examples = %d\", len(train_examples))\n",
        "      logger.info(\"  Batch size = %d\", train_batch_size)\n",
        "      logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "      all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "      all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "      all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "      all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
        "      train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "      if local_rank == -1:\n",
        "          train_sampler = RandomSampler(train_data)\n",
        "      else:\n",
        "          train_sampler = DistributedSampler(train_data)\n",
        "      train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "      model.train()\n",
        "      \n",
        "      for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
        "          tr_loss = 0\n",
        "          nb_tr_examples, nb_tr_steps = 0, 0\n",
        "          for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "              batch = tuple(t.to(device) for t in batch)\n",
        "              input_ids, input_mask, segment_ids, label_ids = batch\n",
        "              loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "              if n_gpu > 1:\n",
        "                  loss = loss.mean() # mean() to average on multi-gpu.\n",
        "              if gradient_accumulation_steps > 1:\n",
        "                  loss = loss / gradient_accumulation_step\n",
        "              if fp16:\n",
        "                  optimizer.backward(loss)\n",
        "              else:\n",
        "                  loss.backward()  \n",
        "              tr_loss += loss.item()\n",
        "              nb_tr_examples += input_ids.size(0)\n",
        "              nb_tr_steps += 1\n",
        "              if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                  # modificar el learning rate con un especial uso warm up de BERT\n",
        "                  lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
        "                  for param_group in optimizer.param_groups:\n",
        "                      param_group['lr'] = lr_this_step\n",
        "                  optimizer.step()\n",
        "                  optimizer.zero_grad()\n",
        "                  global_step += 1\n",
        "   \n",
        "  # Save a trained model\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "  output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
        "  if do_train:\n",
        "      torch.save(model_to_save.state_dict(), output_model_file)\n",
        "  \n",
        "  # Load a trained model that you have fine-tuned\n",
        "  model_state_dict = torch.load(output_model_file)\n",
        "  model = BertForSequenceClassification.from_pretrained(bert_model, state_dict=model_state_dict, num_labels=num_labels)\n",
        "  model.to(device)\n",
        "  \n",
        "  if do_eval and (local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "      eval_examples = processor.get_dev_examples(data_dir)\n",
        "      eval_features = convert_examples_to_features(\n",
        "          eval_examples, label_list, max_seq_length, tokenizer)\n",
        "      logger.info(\"***** Running evaluation *****\")\n",
        "      logger.info(\"  Num examples = %d\", len(eval_examples))\n",
        "      logger.info(\"  Batch size = %d\", eval_batch_size)\n",
        "      all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "      all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "      all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "      all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
        "      eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "      # Run prediction for full data\n",
        "      eval_sampler = SequentialSampler(eval_data)\n",
        "      eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "\n",
        "      model.eval()\n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "      for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "          input_ids = input_ids.to(device)\n",
        "          input_mask = input_mask.to(device)\n",
        "          segment_ids = segment_ids.to(device)\n",
        "          label_ids = label_ids.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
        "              logits = model(input_ids, segment_ids, input_mask)\n",
        "\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          label_ids = label_ids.to('cpu').numpy()\n",
        "          tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "\n",
        "          eval_loss += tmp_eval_loss.mean().item()\n",
        "          eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "          nb_eval_examples += input_ids.size(0)\n",
        "          nb_eval_steps += 1\n",
        "\n",
        "      eval_loss = eval_loss / nb_eval_steps\n",
        "      eval_accuracy = eval_accuracy / nb_eval_examples\n",
        "      loss = tr_loss/nb_tr_steps if do_train else None\n",
        "      result = {'eval_loss': eval_loss,\n",
        "                'eval_accuracy': eval_accuracy,\n",
        "                'global_step': global_step,\n",
        "                'loss': loss}\n",
        "\n",
        "#     input_ids = pd.Series(np.squeeze(input_ids.detach().cpu().numpy()))\n",
        "#     pred = pd.Series(np.squeeze(np.argmax(logits, axis=1)))\n",
        "#     sub = pd.DataFrame({\"id\":input_ids,\"pred\":pred})\n",
        "#     sub.to_csv(os.path.join(args.output_dir, \"submission.csv\"),header=None,index=False)\n",
        "      print(np.squeeze(input_ids.detach().cpu().numpy()).shape)\n",
        "      print(logits.shape)\n",
        "      output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "      with open(output_eval_file, \"w\") as writer:\n",
        "          logger.info(\"***** Eval results *****\")\n",
        "          for key in sorted(result.keys()):\n",
        "              logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "              writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "              \n",
        "main() # llamado al main principal"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "06/14/2019 16:18:04 - INFO - __main__ -   device: cpu n_gpu: 0, distributed training: False, 16-bits training: False\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "task_name:  one\n",
            "do_train:  True\n",
            "do_eval:  True\n",
            "do_lower_case:  True\n",
            "data_dir:  /gdrive/My Drive/SemEval2019/training\n",
            "bert_model:  bert-base-uncased\n",
            "max_seq_length:  80\n",
            "train_batch_size:  32\n",
            "learning_rate:  5e-05\n",
            "num_train_epochs:  2\n",
            "output_dir:  /gdrive/My Drive/SemEval2019/output/\n",
            "prin label_list:  ['0', '1']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "06/14/2019 16:18:05 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmp_phtsf9f\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 1224980.30B/s]\n",
            "06/14/2019 16:18:05 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmp_phtsf9f to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "06/14/2019 16:18:05 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "06/14/2019 16:18:05 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmp_phtsf9f\n",
            "06/14/2019 16:18:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "06/14/2019 16:18:07 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpgeutsfcm\n",
            "100%|██████████| 407873900/407873900 [00:11<00:00, 35370125.97B/s]\n",
            "06/14/2019 16:18:19 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpgeutsfcm to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "06/14/2019 16:18:20 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "06/14/2019 16:18:20 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpgeutsfcm\n",
            "06/14/2019 16:18:20 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "06/14/2019 16:18:20 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp3fd_styd\n",
            "06/14/2019 16:18:25 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "06/14/2019 16:18:29 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "06/14/2019 16:18:29 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   guid: id\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   tokens: [CLS] t ##wee ##t [SEP]\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_ids: 101 1056 28394 2102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   label: 0 (id = 0)\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   guid: 86426\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   tokens: [CLS] @ user she should ask a few native americans what their take on this is . [SEP]\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_ids: 101 1030 5310 2016 2323 3198 1037 2261 3128 4841 2054 2037 2202 2006 2023 2003 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   label: 1 (id = 1)\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   guid: 90194\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   tokens: [CLS] @ user @ user go home you ’ re drunk ! ! ! @ user # mag ##a # trump ##20 ##20 [UNK] ur ##l [SEP]\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_ids: 101 1030 5310 1030 5310 2175 2188 2017 1521 2128 7144 999 999 999 1030 5310 1001 23848 2050 1001 8398 11387 11387 100 24471 2140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   label: 1 (id = 1)\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   guid: 16820\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   tokens: [CLS] amazon is investigating chinese employees who are selling internal data to third - party sellers looking for an edge in the competitive marketplace . ur ##l # amazon # mag ##a # ka ##g # china # tc ##ot [SEP]\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_ids: 101 9733 2003 11538 2822 5126 2040 2024 4855 4722 2951 2000 2353 1011 2283 19041 2559 2005 2019 3341 1999 1996 6975 18086 1012 24471 2140 1001 9733 1001 23848 2050 1001 10556 2290 1001 2859 1001 22975 4140 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   label: 0 (id = 0)\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   guid: 62688\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   tokens: [CLS] \" @ user someone should ' vet ##ake ##n \" \" this piece of shit to a volcano . [UNK] \" \" \" [SEP]\n",
            "06/14/2019 16:18:29 - INFO - __main__ -   input_ids: 101 1000 1030 5310 2619 2323 1005 29525 13808 2078 1000 1000 2023 3538 1997 4485 2000 1037 12779 1012 100 1000 1000 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:30 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:30 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 16:18:30 - INFO - __main__ -   label: 1 (id = 1)\n",
            "06/14/2019 16:18:37 - INFO - __main__ -   ***** Running training *****\n",
            "06/14/2019 16:18:37 - INFO - __main__ -     Num examples = 13241\n",
            "06/14/2019 16:18:37 - INFO - __main__ -     Batch size = 32\n",
            "06/14/2019 16:18:37 - INFO - __main__ -     Num steps = 827\n",
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/414 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 1/414 [00:31<3:39:43, 31.92s/it]\u001b[A\n",
            "Iteration:   0%|          | 2/414 [01:01<3:35:04, 31.32s/it]\u001b[A\n",
            "Iteration:   1%|          | 3/414 [01:30<3:30:03, 30.66s/it]\u001b[A\n",
            "Iteration:   1%|          | 4/414 [02:00<3:26:15, 30.19s/it]\u001b[A\n",
            "Iteration:   1%|          | 5/414 [02:29<3:23:16, 29.82s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 6/414 [02:58<3:21:15, 29.60s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 7/414 [03:27<3:19:35, 29.42s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 8/414 [03:56<3:18:19, 29.31s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 9/414 [04:25<3:17:02, 29.19s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 10/414 [04:54<3:16:08, 29.13s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 11/414 [05:23<3:15:31, 29.11s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 12/414 [05:52<3:15:00, 29.11s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 13/414 [06:21<3:14:27, 29.10s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 14/414 [06:50<3:13:51, 29.08s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 15/414 [07:19<3:13:16, 29.06s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 16/414 [07:48<3:12:38, 29.04s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 17/414 [08:17<3:11:54, 29.00s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 18/414 [08:46<3:11:15, 28.98s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 19/414 [09:15<3:10:49, 28.99s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 20/414 [09:44<3:10:16, 28.98s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 21/414 [10:13<3:09:35, 28.95s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 22/414 [10:42<3:09:34, 29.02s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 23/414 [11:11<3:09:11, 29.03s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 24/414 [11:40<3:08:57, 29.07s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 25/414 [12:09<3:08:19, 29.05s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 26/414 [12:38<3:07:47, 29.04s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 27/414 [13:07<3:07:16, 29.04s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 28/414 [13:36<3:06:31, 28.99s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 29/414 [14:05<3:06:00, 28.99s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 30/414 [14:34<3:05:29, 28.98s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 31/414 [15:03<3:04:48, 28.95s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 32/414 [15:32<3:04:20, 28.95s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 33/414 [16:01<3:04:07, 29.00s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 34/414 [16:30<3:03:36, 28.99s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 35/414 [16:59<3:03:05, 28.99s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 36/414 [17:28<3:02:17, 28.94s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 37/414 [17:56<3:01:47, 28.93s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 38/414 [18:26<3:01:57, 29.04s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 39/414 [18:55<3:01:25, 29.03s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 40/414 [19:24<3:00:39, 28.98s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 41/414 [19:53<3:00:14, 28.99s/it]\u001b[A\n",
            "Iteration:  10%|█         | 42/414 [20:22<2:59:48, 29.00s/it]\u001b[A\n",
            "Iteration:  10%|█         | 43/414 [20:51<2:59:21, 29.01s/it]\u001b[A\n",
            "Iteration:  11%|█         | 44/414 [21:20<2:58:59, 29.02s/it]\u001b[A\n",
            "Iteration:  11%|█         | 45/414 [21:49<2:58:42, 29.06s/it]\u001b[A\n",
            "Iteration:  11%|█         | 46/414 [22:18<2:58:03, 29.03s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 47/414 [22:47<2:57:49, 29.07s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 48/414 [23:16<2:57:10, 29.04s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 49/414 [23:45<2:56:38, 29.04s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 50/414 [24:14<2:55:47, 28.98s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 51/414 [24:43<2:55:10, 28.95s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 52/414 [25:12<2:54:25, 28.91s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 53/414 [25:40<2:53:28, 28.83s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 54/414 [26:09<2:53:13, 28.87s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 55/414 [26:38<2:53:03, 28.92s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 56/414 [27:07<2:53:00, 29.00s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 57/414 [27:36<2:52:35, 29.01s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 58/414 [28:05<2:52:10, 29.02s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 59/414 [28:34<2:51:40, 29.01s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 60/414 [29:03<2:51:04, 29.00s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 61/414 [29:32<2:50:35, 29.00s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 62/414 [30:01<2:50:08, 29.00s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 63/414 [30:30<2:49:34, 28.99s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 64/414 [30:59<2:49:04, 28.98s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 65/414 [31:28<2:48:43, 29.01s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 66/414 [31:57<2:48:14, 29.01s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 67/414 [32:27<2:48:01, 29.05s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 68/414 [32:56<2:47:31, 29.05s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 69/414 [33:25<2:46:57, 29.04s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 70/414 [33:54<2:46:24, 29.02s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 71/414 [34:23<2:45:51, 29.01s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 72/414 [34:52<2:45:12, 28.98s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 73/414 [35:21<2:44:45, 28.99s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 74/414 [35:50<2:44:28, 29.02s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 75/414 [36:19<2:44:06, 29.04s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 76/414 [36:48<2:43:42, 29.06s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 77/414 [37:17<2:43:24, 29.09s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 78/414 [37:46<2:42:54, 29.09s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 79/414 [38:15<2:42:13, 29.06s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 80/414 [38:44<2:41:33, 29.02s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 81/414 [39:13<2:41:02, 29.02s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 82/414 [39:42<2:40:20, 28.98s/it]\u001b[A\n",
            "Iteration:  20%|██        | 83/414 [40:11<2:39:50, 28.98s/it]\u001b[A\n",
            "Iteration:  20%|██        | 84/414 [40:40<2:39:13, 28.95s/it]\u001b[A\n",
            "Iteration:  21%|██        | 85/414 [41:09<2:38:42, 28.94s/it]\u001b[A\n",
            "Iteration:  21%|██        | 86/414 [41:38<2:38:29, 28.99s/it]\u001b[A\n",
            "Iteration:  21%|██        | 87/414 [42:07<2:38:04, 29.01s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 88/414 [42:36<2:37:34, 29.00s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 89/414 [43:05<2:37:02, 28.99s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 90/414 [43:34<2:36:32, 28.99s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 91/414 [44:03<2:35:57, 28.97s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 92/414 [44:32<2:35:30, 28.98s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 93/414 [45:01<2:35:07, 28.99s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 94/414 [45:30<2:34:35, 28.99s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 95/414 [45:59<2:34:11, 29.00s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 96/414 [46:28<2:33:46, 29.01s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 97/414 [46:57<2:33:24, 29.04s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 98/414 [47:26<2:32:46, 29.01s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 99/414 [47:55<2:32:30, 29.05s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 100/414 [48:24<2:32:00, 29.05s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 101/414 [48:53<2:31:25, 29.03s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 102/414 [49:22<2:30:57, 29.03s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 103/414 [49:51<2:30:20, 29.01s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 104/414 [50:20<2:29:44, 28.98s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 105/414 [50:49<2:29:16, 28.98s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 106/414 [51:18<2:28:45, 28.98s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 107/414 [51:47<2:28:14, 28.97s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 108/414 [52:16<2:27:42, 28.96s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 109/414 [52:45<2:27:16, 28.97s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 110/414 [53:14<2:27:00, 29.02s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 111/414 [53:43<2:26:33, 29.02s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 112/414 [54:12<2:26:06, 29.03s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 113/414 [54:41<2:25:27, 28.99s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 114/414 [55:10<2:24:59, 29.00s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 115/414 [55:39<2:24:26, 28.99s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 116/414 [56:08<2:23:53, 28.97s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 117/414 [56:37<2:23:21, 28.96s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 118/414 [57:06<2:22:57, 28.98s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 119/414 [57:35<2:22:22, 28.96s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 120/414 [58:04<2:21:50, 28.95s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 121/414 [58:33<2:21:30, 28.98s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 122/414 [59:02<2:21:00, 28.97s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 123/414 [59:31<2:20:32, 28.98s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 124/414 [1:00:00<2:20:05, 28.98s/it]\u001b[A\n",
            "Iteration:  30%|███       | 125/414 [1:00:28<2:19:22, 28.94s/it]\u001b[A\n",
            "Iteration:  30%|███       | 126/414 [1:00:57<2:18:49, 28.92s/it]\u001b[A\n",
            "Iteration:  31%|███       | 127/414 [1:01:26<2:18:19, 28.92s/it]\u001b[A\n",
            "Iteration:  31%|███       | 128/414 [1:01:55<2:18:01, 28.96s/it]\u001b[A\n",
            "Iteration:  31%|███       | 129/414 [1:02:24<2:17:42, 28.99s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 130/414 [1:02:53<2:17:13, 28.99s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 131/414 [1:03:22<2:16:53, 29.02s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 132/414 [1:03:51<2:16:30, 29.05s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 133/414 [1:04:21<2:16:05, 29.06s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 134/414 [1:04:50<2:15:34, 29.05s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 135/414 [1:05:19<2:15:04, 29.05s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 136/414 [1:05:48<2:14:25, 29.01s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 137/414 [1:06:16<2:13:53, 29.00s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 138/414 [1:06:45<2:13:22, 28.99s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 139/414 [1:07:15<2:12:57, 29.01s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 140/414 [1:07:44<2:12:31, 29.02s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 141/414 [1:08:13<2:12:12, 29.06s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 142/414 [1:08:42<2:11:45, 29.07s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 143/414 [1:09:11<2:11:20, 29.08s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 144/414 [1:09:40<2:10:46, 29.06s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 145/414 [1:10:09<2:10:18, 29.07s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 146/414 [1:10:38<2:09:43, 29.04s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 147/414 [1:11:07<2:09:11, 29.03s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 148/414 [1:11:36<2:08:46, 29.05s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 149/414 [1:12:05<2:08:20, 29.06s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 150/414 [1:12:34<2:08:09, 29.13s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 151/414 [1:13:04<2:07:39, 29.12s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 152/414 [1:13:33<2:07:08, 29.12s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 153/414 [1:14:02<2:06:39, 29.12s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 154/414 [1:14:31<2:06:01, 29.08s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 155/414 [1:15:00<2:05:29, 29.07s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 156/414 [1:15:29<2:05:01, 29.08s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 157/414 [1:15:58<2:04:21, 29.03s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 158/414 [1:16:27<2:03:44, 29.00s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 159/414 [1:16:56<2:03:08, 28.97s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 160/414 [1:17:25<2:02:38, 28.97s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 161/414 [1:17:54<2:02:04, 28.95s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 162/414 [1:18:23<2:01:41, 28.97s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 163/414 [1:18:52<2:01:15, 28.98s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 164/414 [1:19:21<2:00:48, 29.00s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 165/414 [1:19:50<2:00:16, 28.98s/it]\u001b[A\n",
            "Iteration:  40%|████      | 166/414 [1:20:19<1:59:45, 28.97s/it]\u001b[A\n",
            "Iteration:  40%|████      | 167/414 [1:20:47<1:59:13, 28.96s/it]\u001b[A\n",
            "Iteration:  41%|████      | 168/414 [1:21:16<1:58:50, 28.99s/it]\u001b[A\n",
            "Iteration:  41%|████      | 169/414 [1:21:45<1:58:20, 28.98s/it]\u001b[A\n",
            "Iteration:  41%|████      | 170/414 [1:22:14<1:57:47, 28.97s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 171/414 [1:22:43<1:57:26, 29.00s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 172/414 [1:23:13<1:57:02, 29.02s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 173/414 [1:23:42<1:56:34, 29.02s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 174/414 [1:24:11<1:56:12, 29.05s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 175/414 [1:24:40<1:55:36, 29.02s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 176/414 [1:25:09<1:55:01, 29.00s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 177/414 [1:25:38<1:54:43, 29.04s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 178/414 [1:26:07<1:54:18, 29.06s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 179/414 [1:26:36<1:53:35, 29.00s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 180/414 [1:27:05<1:53:04, 28.99s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 181/414 [1:27:34<1:52:27, 28.96s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 182/414 [1:28:02<1:51:50, 28.92s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 183/414 [1:28:31<1:51:21, 28.92s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 184/414 [1:29:00<1:50:54, 28.93s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 185/414 [1:29:29<1:50:30, 28.95s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 186/414 [1:29:58<1:50:03, 28.96s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 187/414 [1:30:27<1:49:37, 28.98s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 188/414 [1:30:56<1:49:04, 28.96s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 189/414 [1:31:25<1:48:39, 28.98s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 190/414 [1:31:54<1:48:14, 28.99s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 191/414 [1:32:23<1:47:44, 28.99s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 192/414 [1:32:52<1:47:07, 28.95s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 193/414 [1:33:21<1:46:38, 28.95s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 194/414 [1:33:50<1:46:08, 28.95s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 195/414 [1:34:19<1:45:32, 28.92s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 196/414 [1:34:48<1:45:11, 28.95s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 197/414 [1:35:17<1:44:47, 28.97s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 198/414 [1:35:46<1:44:19, 28.98s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 199/414 [1:36:15<1:43:51, 28.98s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 200/414 [1:36:44<1:43:25, 29.00s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 201/414 [1:37:13<1:42:55, 28.99s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 202/414 [1:37:42<1:42:23, 28.98s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 203/414 [1:38:11<1:41:55, 28.98s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 204/414 [1:38:40<1:41:24, 28.97s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 205/414 [1:39:09<1:40:56, 28.98s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 206/414 [1:39:38<1:40:34, 29.01s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 207/414 [1:40:07<1:40:03, 29.00s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 208/414 [1:40:36<1:39:31, 28.99s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 209/414 [1:41:05<1:38:58, 28.97s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 210/414 [1:41:34<1:38:33, 28.99s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 211/414 [1:42:03<1:38:01, 28.97s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 212/414 [1:42:32<1:37:34, 28.98s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 213/414 [1:43:01<1:37:09, 29.00s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 214/414 [1:43:30<1:36:33, 28.97s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 215/414 [1:43:59<1:36:03, 28.96s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 216/414 [1:44:27<1:35:31, 28.94s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 217/414 [1:44:56<1:35:02, 28.95s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 218/414 [1:45:25<1:34:32, 28.94s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 219/414 [1:45:54<1:34:06, 28.96s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 220/414 [1:46:23<1:33:32, 28.93s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 221/414 [1:46:52<1:33:04, 28.94s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 222/414 [1:47:21<1:32:35, 28.94s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 223/414 [1:47:50<1:32:02, 28.91s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 224/414 [1:48:19<1:31:38, 28.94s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 225/414 [1:48:48<1:31:09, 28.94s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 226/414 [1:49:17<1:30:40, 28.94s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 227/414 [1:49:46<1:30:11, 28.94s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 228/414 [1:50:15<1:29:47, 28.96s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 229/414 [1:50:44<1:29:18, 28.97s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 230/414 [1:51:13<1:28:44, 28.94s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 231/414 [1:51:42<1:28:15, 28.94s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 232/414 [1:52:10<1:27:45, 28.93s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 233/414 [1:52:39<1:27:18, 28.94s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 234/414 [1:53:08<1:26:52, 28.96s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 235/414 [1:53:38<1:26:31, 29.00s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 236/414 [1:54:06<1:25:58, 28.98s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 237/414 [1:54:36<1:25:52, 29.11s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 238/414 [1:55:05<1:25:16, 29.07s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 239/414 [1:55:34<1:24:42, 29.05s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 240/414 [1:56:03<1:24:06, 29.00s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 241/414 [1:56:32<1:23:36, 28.99s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 242/414 [1:57:01<1:23:07, 29.00s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 243/414 [1:57:30<1:22:43, 29.02s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 244/414 [1:57:59<1:22:11, 29.01s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 245/414 [1:58:28<1:21:37, 28.98s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 246/414 [1:58:57<1:21:08, 28.98s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 247/414 [1:59:26<1:20:40, 28.98s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 248/414 [1:59:55<1:20:10, 28.98s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 249/414 [2:00:24<1:19:37, 28.95s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 250/414 [2:00:53<1:19:12, 28.98s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 251/414 [2:01:22<1:18:41, 28.97s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 252/414 [2:01:50<1:18:11, 28.96s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 253/414 [2:02:19<1:17:42, 28.96s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 254/414 [2:02:48<1:17:15, 28.97s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 255/414 [2:03:17<1:16:47, 28.98s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 256/414 [2:03:47<1:16:27, 29.04s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 257/414 [2:04:16<1:15:54, 29.01s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 258/414 [2:04:44<1:15:21, 28.98s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 259/414 [2:05:13<1:14:46, 28.95s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 260/414 [2:05:42<1:14:21, 28.97s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 261/414 [2:06:11<1:13:51, 28.97s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 262/414 [2:06:40<1:13:29, 29.01s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 263/414 [2:07:10<1:13:06, 29.05s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 264/414 [2:07:39<1:12:34, 29.03s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 265/414 [2:08:08<1:12:03, 29.02s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 266/414 [2:08:37<1:11:36, 29.03s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 267/414 [2:09:06<1:11:06, 29.02s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 268/414 [2:09:35<1:10:35, 29.01s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 269/414 [2:10:04<1:10:08, 29.03s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 270/414 [2:10:33<1:09:39, 29.03s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 271/414 [2:11:02<1:09:09, 29.02s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 272/414 [2:11:31<1:08:40, 29.01s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 273/414 [2:12:00<1:08:13, 29.04s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 274/414 [2:12:29<1:07:43, 29.03s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 275/414 [2:12:58<1:07:13, 29.02s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 276/414 [2:13:27<1:06:41, 29.00s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 277/414 [2:13:56<1:06:13, 29.00s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 278/414 [2:14:25<1:05:46, 29.02s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 279/414 [2:14:54<1:05:18, 29.03s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 280/414 [2:15:23<1:04:47, 29.01s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 281/414 [2:15:52<1:04:22, 29.04s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 282/414 [2:16:21<1:03:58, 29.08s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 283/414 [2:16:50<1:03:21, 29.02s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 284/414 [2:17:19<1:02:49, 29.00s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 285/414 [2:17:48<1:02:20, 29.00s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 286/414 [2:18:17<1:01:48, 28.97s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 287/414 [2:18:46<1:01:21, 28.99s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 288/414 [2:19:15<1:00:54, 29.00s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 289/414 [2:19:44<1:00:22, 28.98s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 290/414 [2:20:13<59:51, 28.96s/it]  \u001b[A\n",
            "Iteration:  70%|███████   | 291/414 [2:20:42<59:22, 28.96s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 292/414 [2:21:11<58:52, 28.96s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 293/414 [2:21:40<58:26, 28.98s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 294/414 [2:22:09<57:58, 28.99s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 295/414 [2:22:38<57:26, 28.96s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 296/414 [2:23:07<56:57, 28.96s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 297/414 [2:23:35<56:29, 28.97s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 298/414 [2:24:05<56:02, 28.99s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 299/414 [2:24:33<55:31, 28.97s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 300/414 [2:25:02<55:02, 28.97s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 301/414 [2:25:31<54:32, 28.96s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 302/414 [2:26:00<54:04, 28.97s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 303/414 [2:26:29<53:39, 29.00s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 304/414 [2:26:58<53:11, 29.01s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 305/414 [2:27:27<52:40, 29.00s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 306/414 [2:27:56<52:11, 28.99s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 307/414 [2:28:25<51:42, 28.99s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 308/414 [2:28:54<51:11, 28.97s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 309/414 [2:29:23<50:42, 28.98s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 310/414 [2:29:52<50:12, 28.97s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 311/414 [2:30:21<49:41, 28.94s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 312/414 [2:30:50<49:13, 28.96s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 313/414 [2:31:19<48:45, 28.97s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 314/414 [2:31:48<48:17, 28.97s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 315/414 [2:32:17<47:47, 28.97s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 316/414 [2:32:46<47:18, 28.96s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 317/414 [2:33:15<46:46, 28.93s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 318/414 [2:33:44<46:17, 28.93s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 319/414 [2:34:13<45:49, 28.94s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 320/414 [2:34:42<45:21, 28.95s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 321/414 [2:35:11<44:53, 28.96s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 322/414 [2:35:40<44:25, 28.97s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 323/414 [2:36:09<43:56, 28.98s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 324/414 [2:36:38<43:24, 28.94s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 325/414 [2:37:07<42:58, 28.97s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 326/414 [2:37:36<42:29, 28.97s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 327/414 [2:38:05<41:59, 28.95s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 328/414 [2:38:33<41:28, 28.94s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 329/414 [2:39:02<41:00, 28.95s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 330/414 [2:39:31<40:33, 28.98s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 331/414 [2:40:00<40:04, 28.98s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 332/414 [2:40:29<39:34, 28.96s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 333/414 [2:40:58<39:02, 28.92s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 334/414 [2:41:27<38:32, 28.90s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 335/414 [2:41:56<38:04, 28.91s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 336/414 [2:42:25<37:35, 28.92s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 337/414 [2:42:54<37:08, 28.94s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 338/414 [2:43:23<36:41, 28.97s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 339/414 [2:43:52<36:11, 28.95s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 340/414 [2:44:21<35:42, 28.95s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 341/414 [2:44:50<35:13, 28.95s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 342/414 [2:45:19<34:44, 28.95s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 343/414 [2:45:48<34:14, 28.93s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 344/414 [2:46:16<33:45, 28.93s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 345/414 [2:46:45<33:15, 28.92s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 346/414 [2:47:14<32:48, 28.95s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 347/414 [2:47:43<32:20, 28.96s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 348/414 [2:48:12<31:52, 28.98s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 349/414 [2:48:41<31:22, 28.96s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 350/414 [2:49:10<30:55, 29.00s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 351/414 [2:49:39<30:25, 28.97s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 352/414 [2:50:08<29:55, 28.95s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 353/414 [2:50:37<29:25, 28.94s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 354/414 [2:51:06<28:56, 28.94s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 355/414 [2:51:35<28:27, 28.94s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 356/414 [2:52:04<27:59, 28.95s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 357/414 [2:52:33<27:32, 29.00s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 358/414 [2:53:02<27:02, 28.97s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 359/414 [2:53:31<26:34, 28.99s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 360/414 [2:54:00<26:04, 28.97s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 361/414 [2:54:29<25:34, 28.96s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 362/414 [2:54:58<25:04, 28.93s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 363/414 [2:55:27<24:35, 28.94s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 364/414 [2:55:56<24:07, 28.94s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 365/414 [2:56:25<23:37, 28.93s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 366/414 [2:56:53<23:08, 28.93s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 367/414 [2:57:22<22:39, 28.93s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 368/414 [2:57:51<22:10, 28.93s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 369/414 [2:58:20<21:43, 28.97s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 370/414 [2:58:49<21:13, 28.94s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 371/414 [2:59:18<20:43, 28.93s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 372/414 [2:59:47<20:14, 28.92s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 373/414 [3:00:16<19:46, 28.93s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 374/414 [3:00:45<19:16, 28.92s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 375/414 [3:01:14<18:48, 28.94s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 376/414 [3:01:43<18:20, 28.96s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 377/414 [3:02:12<17:51, 28.95s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 378/414 [3:02:41<17:22, 28.97s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 379/414 [3:03:10<16:54, 28.99s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 380/414 [3:03:39<16:25, 28.97s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 381/414 [3:04:08<15:54, 28.93s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 382/414 [3:04:37<15:25, 28.92s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 383/414 [3:05:05<14:56, 28.92s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 384/414 [3:05:34<14:27, 28.91s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 385/414 [3:06:03<13:58, 28.92s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 386/414 [3:06:32<13:29, 28.91s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 387/414 [3:07:01<13:00, 28.91s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 388/414 [3:07:30<12:32, 28.95s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 389/414 [3:07:59<12:03, 28.95s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 390/414 [3:08:28<11:35, 28.96s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 391/414 [3:08:57<11:05, 28.95s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 392/414 [3:09:26<10:36, 28.94s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 393/414 [3:09:55<10:07, 28.92s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 394/414 [3:10:24<09:39, 28.95s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 395/414 [3:10:53<09:09, 28.94s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 396/414 [3:11:22<08:40, 28.91s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 397/414 [3:11:50<08:11, 28.91s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 398/414 [3:12:19<07:42, 28.93s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 399/414 [3:12:48<07:13, 28.93s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 400/414 [3:13:18<06:45, 28.99s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 401/414 [3:13:46<06:16, 28.97s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 402/414 [3:14:15<05:47, 28.94s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 403/414 [3:14:44<05:17, 28.89s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 404/414 [3:15:13<04:49, 28.90s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 405/414 [3:15:42<04:20, 28.94s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 406/414 [3:16:11<03:51, 28.93s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 407/414 [3:16:40<03:22, 28.96s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 408/414 [3:17:09<02:53, 28.95s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 409/414 [3:17:38<02:24, 28.93s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 410/414 [3:18:07<01:55, 28.92s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 411/414 [3:18:36<01:26, 28.95s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 412/414 [3:19:05<00:57, 28.94s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 413/414 [3:19:34<00:28, 28.98s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 414/414 [3:19:57<00:00, 27.31s/it]\u001b[A\n",
            "Epoch:  50%|█████     | 1/2 [3:19:57<3:19:57, 11997.61s/it]\n",
            "Iteration:   0%|          | 0/414 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 1/414 [00:28<3:19:16, 28.95s/it]\u001b[A\n",
            "Iteration:   0%|          | 2/414 [00:57<3:18:55, 28.97s/it]\u001b[A\n",
            "Iteration:   1%|          | 3/414 [01:26<3:18:27, 28.97s/it]\u001b[A\n",
            "Iteration:   1%|          | 4/414 [01:55<3:17:51, 28.96s/it]\u001b[A\n",
            "Iteration:   1%|          | 5/414 [02:24<3:17:19, 28.95s/it]\u001b[A\n",
            "Iteration:   1%|▏         | 6/414 [02:53<3:16:44, 28.93s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 7/414 [03:22<3:16:24, 28.95s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 8/414 [03:51<3:15:58, 28.96s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 9/414 [04:20<3:15:27, 28.96s/it]\u001b[A\n",
            "Iteration:   2%|▏         | 10/414 [04:49<3:14:55, 28.95s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 11/414 [05:18<3:14:10, 28.91s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 12/414 [05:47<3:13:46, 28.92s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 13/414 [06:16<3:13:02, 28.88s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 14/414 [06:44<3:12:32, 28.88s/it]\u001b[A\n",
            "Iteration:   4%|▎         | 15/414 [07:13<3:12:10, 28.90s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 16/414 [07:42<3:11:51, 28.92s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 17/414 [08:11<3:11:23, 28.93s/it]\u001b[A\n",
            "Iteration:   4%|▍         | 18/414 [08:40<3:11:00, 28.94s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 19/414 [09:09<3:10:45, 28.98s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 20/414 [09:38<3:10:04, 28.94s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 21/414 [10:07<3:09:41, 28.96s/it]\u001b[A\n",
            "Iteration:   5%|▌         | 22/414 [10:36<3:09:17, 28.97s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 23/414 [11:05<3:08:46, 28.97s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 24/414 [11:34<3:08:18, 28.97s/it]\u001b[A\n",
            "Iteration:   6%|▌         | 25/414 [12:03<3:07:48, 28.97s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 26/414 [12:32<3:07:08, 28.94s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 27/414 [13:01<3:06:27, 28.91s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 28/414 [13:30<3:05:58, 28.91s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 29/414 [13:59<3:05:42, 28.94s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 30/414 [14:28<3:05:21, 28.96s/it]\u001b[A\n",
            "Iteration:   7%|▋         | 31/414 [14:57<3:04:52, 28.96s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 32/414 [15:26<3:04:22, 28.96s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 33/414 [15:55<3:04:05, 28.99s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 34/414 [16:24<3:03:47, 29.02s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 35/414 [16:53<3:03:07, 28.99s/it]\u001b[A\n",
            "Iteration:   9%|▊         | 36/414 [17:22<3:02:31, 28.97s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 37/414 [17:51<3:02:01, 28.97s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 38/414 [18:20<3:01:32, 28.97s/it]\u001b[A\n",
            "Iteration:   9%|▉         | 39/414 [18:49<3:00:59, 28.96s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 40/414 [19:18<3:00:46, 29.00s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 41/414 [19:47<3:00:16, 29.00s/it]\u001b[A\n",
            "Iteration:  10%|█         | 42/414 [20:16<2:59:31, 28.95s/it]\u001b[A\n",
            "Iteration:  10%|█         | 43/414 [20:44<2:59:00, 28.95s/it]\u001b[A\n",
            "Iteration:  11%|█         | 44/414 [21:13<2:58:40, 28.98s/it]\u001b[A\n",
            "Iteration:  11%|█         | 45/414 [21:42<2:57:58, 28.94s/it]\u001b[A\n",
            "Iteration:  11%|█         | 46/414 [22:11<2:57:32, 28.95s/it]\u001b[A\n",
            "Iteration:  11%|█▏        | 47/414 [22:40<2:57:03, 28.95s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 48/414 [23:09<2:56:23, 28.92s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 49/414 [23:38<2:56:02, 28.94s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 50/414 [24:07<2:55:37, 28.95s/it]\u001b[A\n",
            "Iteration:  12%|█▏        | 51/414 [24:36<2:55:02, 28.93s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 52/414 [25:05<2:54:28, 28.92s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 53/414 [25:34<2:53:59, 28.92s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 54/414 [26:03<2:53:17, 28.88s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 55/414 [26:31<2:52:46, 28.88s/it]\u001b[A\n",
            "Iteration:  14%|█▎        | 56/414 [27:00<2:52:20, 28.89s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 57/414 [27:29<2:51:50, 28.88s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 58/414 [27:58<2:51:32, 28.91s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 59/414 [28:27<2:51:10, 28.93s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 60/414 [28:56<2:50:47, 28.95s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 61/414 [29:25<2:50:16, 28.94s/it]\u001b[A\n",
            "Iteration:  15%|█▍        | 62/414 [29:54<2:49:55, 28.96s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 63/414 [30:23<2:49:22, 28.95s/it]\u001b[A\n",
            "Iteration:  15%|█▌        | 64/414 [30:52<2:48:41, 28.92s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 65/414 [31:21<2:48:15, 28.93s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 66/414 [31:50<2:47:56, 28.95s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 67/414 [32:19<2:47:24, 28.95s/it]\u001b[A\n",
            "Iteration:  16%|█▋        | 68/414 [32:48<2:46:58, 28.96s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 69/414 [33:17<2:46:46, 29.00s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 70/414 [33:46<2:46:27, 29.03s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 71/414 [34:15<2:45:47, 29.00s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 72/414 [34:44<2:45:20, 29.01s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 73/414 [35:13<2:44:46, 28.99s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 74/414 [35:42<2:44:10, 28.97s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 75/414 [36:11<2:43:40, 28.97s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 76/414 [36:40<2:43:20, 29.00s/it]\u001b[A\n",
            "Iteration:  19%|█▊        | 77/414 [37:09<2:42:57, 29.01s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 78/414 [37:38<2:42:26, 29.01s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 79/414 [38:07<2:41:51, 28.99s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 80/414 [38:36<2:41:12, 28.96s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 81/414 [39:05<2:40:43, 28.96s/it]\u001b[A\n",
            "Iteration:  20%|█▉        | 82/414 [39:34<2:40:12, 28.95s/it]\u001b[A\n",
            "Iteration:  20%|██        | 83/414 [40:02<2:39:34, 28.93s/it]\u001b[A\n",
            "Iteration:  20%|██        | 84/414 [40:31<2:39:06, 28.93s/it]\u001b[A\n",
            "Iteration:  21%|██        | 85/414 [41:00<2:38:36, 28.93s/it]\u001b[A\n",
            "Iteration:  21%|██        | 86/414 [41:29<2:38:00, 28.91s/it]\u001b[A\n",
            "Iteration:  21%|██        | 87/414 [41:58<2:37:38, 28.93s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 88/414 [42:27<2:37:11, 28.93s/it]\u001b[A\n",
            "Iteration:  21%|██▏       | 89/414 [42:56<2:36:25, 28.88s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 90/414 [43:25<2:36:01, 28.89s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 91/414 [43:54<2:35:30, 28.89s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 92/414 [44:23<2:35:11, 28.92s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 93/414 [44:52<2:34:51, 28.95s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 94/414 [45:21<2:34:37, 28.99s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 95/414 [45:50<2:33:56, 28.95s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 96/414 [46:18<2:33:20, 28.93s/it]\u001b[A\n",
            "Iteration:  23%|██▎       | 97/414 [46:47<2:32:51, 28.93s/it]\u001b[A\n",
            "Iteration:  24%|██▎       | 98/414 [47:16<2:32:26, 28.95s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 99/414 [47:45<2:31:41, 28.89s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 100/414 [48:14<2:31:11, 28.89s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 101/414 [48:43<2:30:38, 28.88s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 102/414 [49:12<2:30:06, 28.87s/it]\u001b[A\n",
            "Iteration:  25%|██▍       | 103/414 [49:41<2:29:37, 28.87s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 104/414 [50:10<2:29:13, 28.88s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 105/414 [50:38<2:28:49, 28.90s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 106/414 [51:07<2:28:24, 28.91s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 107/414 [51:36<2:28:03, 28.94s/it]\u001b[A\n",
            "Iteration:  26%|██▌       | 108/414 [52:06<2:27:54, 29.00s/it]\u001b[A\n",
            "Iteration:  26%|██▋       | 109/414 [52:34<2:27:11, 28.96s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 110/414 [53:03<2:26:40, 28.95s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 111/414 [53:32<2:26:03, 28.92s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 112/414 [54:01<2:25:38, 28.93s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 113/414 [54:30<2:25:10, 28.94s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 114/414 [54:59<2:24:40, 28.93s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 115/414 [55:28<2:24:15, 28.95s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 116/414 [55:57<2:23:59, 28.99s/it]\u001b[A\n",
            "Iteration:  28%|██▊       | 117/414 [56:26<2:23:30, 28.99s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 118/414 [56:55<2:22:48, 28.95s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 119/414 [57:24<2:22:56, 29.07s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 120/414 [57:54<2:23:37, 29.31s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 121/414 [58:23<2:22:33, 29.19s/it]\u001b[A\n",
            "Iteration:  29%|██▉       | 122/414 [58:53<2:22:32, 29.29s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 123/414 [59:22<2:21:38, 29.20s/it]\u001b[A\n",
            "Iteration:  30%|██▉       | 124/414 [59:50<2:20:39, 29.10s/it]\u001b[A\n",
            "Iteration:  30%|███       | 125/414 [1:00:19<2:20:04, 29.08s/it]\u001b[A\n",
            "Iteration:  30%|███       | 126/414 [1:00:48<2:19:14, 29.01s/it]\u001b[A\n",
            "Iteration:  31%|███       | 127/414 [1:01:17<2:18:35, 28.97s/it]\u001b[A\n",
            "Iteration:  31%|███       | 128/414 [1:01:46<2:18:22, 29.03s/it]\u001b[A\n",
            "Iteration:  31%|███       | 129/414 [1:02:15<2:17:56, 29.04s/it]\u001b[A\n",
            "Iteration:  31%|███▏      | 130/414 [1:02:44<2:17:27, 29.04s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 131/414 [1:03:14<2:17:06, 29.07s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 132/414 [1:03:43<2:16:46, 29.10s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 133/414 [1:04:12<2:16:12, 29.08s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 134/414 [1:04:41<2:15:34, 29.05s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 135/414 [1:05:10<2:15:18, 29.10s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 136/414 [1:05:39<2:14:40, 29.07s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 137/414 [1:06:08<2:14:09, 29.06s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 138/414 [1:06:38<2:14:19, 29.20s/it]\u001b[A\n",
            "Iteration:  34%|███▎      | 139/414 [1:07:07<2:13:29, 29.12s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 140/414 [1:07:36<2:13:01, 29.13s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 141/414 [1:08:05<2:13:11, 29.27s/it]\u001b[A\n",
            "Iteration:  34%|███▍      | 142/414 [1:08:34<2:12:05, 29.14s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 143/414 [1:09:03<2:11:34, 29.13s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 144/414 [1:09:32<2:10:59, 29.11s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 145/414 [1:10:01<2:10:09, 29.03s/it]\u001b[A\n",
            "Iteration:  35%|███▌      | 146/414 [1:10:31<2:10:43, 29.27s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 147/414 [1:11:00<2:09:49, 29.17s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 148/414 [1:11:29<2:08:58, 29.09s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 149/414 [1:11:58<2:08:45, 29.15s/it]\u001b[A\n",
            "Iteration:  36%|███▌      | 150/414 [1:12:28<2:08:51, 29.28s/it]\u001b[A\n",
            "Iteration:  36%|███▋      | 151/414 [1:12:57<2:08:20, 29.28s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 152/414 [1:13:26<2:07:26, 29.18s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 153/414 [1:13:56<2:07:40, 29.35s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 154/414 [1:14:25<2:07:29, 29.42s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 155/414 [1:14:54<2:06:12, 29.24s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 156/414 [1:15:23<2:05:20, 29.15s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 157/414 [1:15:52<2:04:36, 29.09s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 158/414 [1:16:21<2:04:07, 29.09s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 159/414 [1:16:50<2:03:31, 29.06s/it]\u001b[A\n",
            "Iteration:  39%|███▊      | 160/414 [1:17:19<2:02:51, 29.02s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 161/414 [1:17:48<2:02:30, 29.05s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 162/414 [1:18:18<2:02:29, 29.16s/it]\u001b[A\n",
            "Iteration:  39%|███▉      | 163/414 [1:18:47<2:02:05, 29.18s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 164/414 [1:19:16<2:01:10, 29.08s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 165/414 [1:19:45<2:00:32, 29.05s/it]\u001b[A\n",
            "Iteration:  40%|████      | 166/414 [1:20:13<1:59:48, 28.99s/it]\u001b[A\n",
            "Iteration:  40%|████      | 167/414 [1:20:42<1:59:06, 28.93s/it]\u001b[A\n",
            "Iteration:  41%|████      | 168/414 [1:21:11<1:58:28, 28.89s/it]\u001b[A\n",
            "Iteration:  41%|████      | 169/414 [1:21:40<1:58:06, 28.92s/it]\u001b[A\n",
            "Iteration:  41%|████      | 170/414 [1:22:09<1:58:16, 29.08s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 171/414 [1:22:39<1:58:14, 29.19s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 172/414 [1:23:08<1:57:27, 29.12s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 173/414 [1:23:37<1:56:49, 29.09s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 174/414 [1:24:06<1:56:06, 29.03s/it]\u001b[A\n",
            "Iteration:  42%|████▏     | 175/414 [1:24:35<1:55:27, 28.99s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 176/414 [1:25:04<1:54:56, 28.97s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 177/414 [1:25:33<1:54:23, 28.96s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 178/414 [1:26:01<1:53:53, 28.95s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 179/414 [1:26:31<1:53:29, 28.98s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 180/414 [1:26:59<1:53:01, 28.98s/it]\u001b[A\n",
            "Iteration:  44%|████▎     | 181/414 [1:27:29<1:52:38, 29.01s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 182/414 [1:27:58<1:52:17, 29.04s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 183/414 [1:28:27<1:51:51, 29.05s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 184/414 [1:28:56<1:51:25, 29.07s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 185/414 [1:29:25<1:50:55, 29.06s/it]\u001b[A\n",
            "Iteration:  45%|████▍     | 186/414 [1:29:54<1:50:22, 29.05s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 187/414 [1:30:23<1:50:06, 29.10s/it]\u001b[A\n",
            "Iteration:  45%|████▌     | 188/414 [1:30:52<1:49:42, 29.13s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 189/414 [1:31:21<1:49:11, 29.12s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 190/414 [1:31:51<1:48:42, 29.12s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 191/414 [1:32:20<1:48:22, 29.16s/it]\u001b[A\n",
            "Iteration:  46%|████▋     | 192/414 [1:32:49<1:47:50, 29.15s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 193/414 [1:33:18<1:47:17, 29.13s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 194/414 [1:33:47<1:46:42, 29.10s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 195/414 [1:34:16<1:46:14, 29.11s/it]\u001b[A\n",
            "Iteration:  47%|████▋     | 196/414 [1:34:45<1:45:41, 29.09s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 197/414 [1:35:14<1:45:08, 29.07s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 198/414 [1:35:43<1:44:36, 29.06s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 199/414 [1:36:12<1:43:57, 29.01s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 200/414 [1:36:41<1:43:22, 28.98s/it]\u001b[A\n",
            "Iteration:  49%|████▊     | 201/414 [1:37:10<1:42:49, 28.96s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 202/414 [1:37:39<1:42:16, 28.95s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 203/414 [1:38:08<1:41:43, 28.93s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 204/414 [1:38:37<1:41:23, 28.97s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 205/414 [1:39:06<1:41:15, 29.07s/it]\u001b[A\n",
            "Iteration:  50%|████▉     | 206/414 [1:39:36<1:41:08, 29.18s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 207/414 [1:40:05<1:40:47, 29.22s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 208/414 [1:40:34<1:40:02, 29.14s/it]\u001b[A\n",
            "Iteration:  50%|█████     | 209/414 [1:41:03<1:39:30, 29.13s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 210/414 [1:41:32<1:39:00, 29.12s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 211/414 [1:42:01<1:38:39, 29.16s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 212/414 [1:42:30<1:38:09, 29.16s/it]\u001b[A\n",
            "Iteration:  51%|█████▏    | 213/414 [1:42:59<1:37:28, 29.10s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 214/414 [1:43:28<1:36:55, 29.08s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 215/414 [1:43:57<1:36:11, 29.00s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 216/414 [1:44:26<1:35:38, 28.98s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 217/414 [1:44:55<1:35:05, 28.96s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 218/414 [1:45:24<1:34:27, 28.92s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 219/414 [1:45:53<1:33:59, 28.92s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 220/414 [1:46:22<1:33:26, 28.90s/it]\u001b[A\n",
            "Iteration:  53%|█████▎    | 221/414 [1:46:51<1:33:00, 28.91s/it]\u001b[A\n",
            "Iteration:  54%|█████▎    | 222/414 [1:47:20<1:32:32, 28.92s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 223/414 [1:47:49<1:32:10, 28.96s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 224/414 [1:48:18<1:31:41, 28.96s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 225/414 [1:48:47<1:31:21, 29.00s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 226/414 [1:49:16<1:30:51, 29.00s/it]\u001b[A\n",
            "Iteration:  55%|█████▍    | 227/414 [1:49:45<1:30:15, 28.96s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 228/414 [1:50:14<1:29:45, 28.95s/it]\u001b[A\n",
            "Iteration:  55%|█████▌    | 229/414 [1:50:43<1:29:19, 28.97s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 230/414 [1:51:11<1:28:41, 28.92s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 231/414 [1:51:40<1:28:09, 28.90s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 232/414 [1:52:09<1:27:37, 28.89s/it]\u001b[A\n",
            "Iteration:  56%|█████▋    | 233/414 [1:52:38<1:27:06, 28.88s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 234/414 [1:53:07<1:26:39, 28.88s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 235/414 [1:53:36<1:26:08, 28.87s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 236/414 [1:54:04<1:25:33, 28.84s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 237/414 [1:54:33<1:24:59, 28.81s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 238/414 [1:55:02<1:24:29, 28.80s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 239/414 [1:55:31<1:24:00, 28.81s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 240/414 [1:56:00<1:23:32, 28.81s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 241/414 [1:56:28<1:23:06, 28.83s/it]\u001b[A\n",
            "Iteration:  58%|█████▊    | 242/414 [1:56:57<1:22:39, 28.83s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 243/414 [1:57:26<1:22:06, 28.81s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 244/414 [1:57:55<1:21:40, 28.82s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 245/414 [1:58:24<1:21:14, 28.84s/it]\u001b[A\n",
            "Iteration:  59%|█████▉    | 246/414 [1:58:53<1:20:45, 28.84s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 247/414 [1:59:22<1:20:19, 28.86s/it]\u001b[A\n",
            "Iteration:  60%|█████▉    | 248/414 [1:59:50<1:19:51, 28.86s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 249/414 [2:00:19<1:19:17, 28.83s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 250/414 [2:00:48<1:18:47, 28.83s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 251/414 [2:01:17<1:18:15, 28.81s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 252/414 [2:01:45<1:17:45, 28.80s/it]\u001b[A\n",
            "Iteration:  61%|██████    | 253/414 [2:02:14<1:17:15, 28.79s/it]\u001b[A\n",
            "Iteration:  61%|██████▏   | 254/414 [2:02:43<1:16:52, 28.83s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 255/414 [2:03:12<1:16:24, 28.84s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 256/414 [2:03:41<1:16:00, 28.87s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 257/414 [2:04:10<1:15:39, 28.92s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 258/414 [2:04:39<1:15:13, 28.93s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 259/414 [2:05:08<1:14:42, 28.92s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 260/414 [2:05:37<1:14:10, 28.90s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 261/414 [2:06:06<1:13:40, 28.89s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 262/414 [2:06:35<1:13:12, 28.90s/it]\u001b[A\n",
            "Iteration:  64%|██████▎   | 263/414 [2:07:03<1:12:47, 28.93s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 264/414 [2:07:32<1:12:18, 28.92s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 265/414 [2:08:01<1:11:47, 28.91s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 266/414 [2:08:30<1:11:24, 28.95s/it]\u001b[A\n",
            "Iteration:  64%|██████▍   | 267/414 [2:08:59<1:11:02, 29.00s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 268/414 [2:09:28<1:10:35, 29.01s/it]\u001b[A\n",
            "Iteration:  65%|██████▍   | 269/414 [2:09:57<1:10:05, 29.00s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 270/414 [2:10:26<1:09:34, 28.99s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 271/414 [2:10:55<1:08:59, 28.94s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 272/414 [2:11:24<1:08:27, 28.93s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 273/414 [2:11:53<1:08:14, 29.04s/it]\u001b[A\n",
            "Iteration:  66%|██████▌   | 274/414 [2:12:22<1:07:41, 29.01s/it]\u001b[A\n",
            "Iteration:  66%|██████▋   | 275/414 [2:12:51<1:07:10, 29.00s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 276/414 [2:13:20<1:06:42, 29.01s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 277/414 [2:13:49<1:06:13, 29.00s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 278/414 [2:14:19<1:05:49, 29.04s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 279/414 [2:14:48<1:05:20, 29.04s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 280/414 [2:15:17<1:04:53, 29.05s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 281/414 [2:15:46<1:04:24, 29.06s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 282/414 [2:16:15<1:03:54, 29.05s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 283/414 [2:16:44<1:03:23, 29.03s/it]\u001b[A\n",
            "Iteration:  69%|██████▊   | 284/414 [2:17:13<1:03:15, 29.20s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 285/414 [2:17:43<1:02:52, 29.24s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 286/414 [2:18:12<1:02:18, 29.21s/it]\u001b[A\n",
            "Iteration:  69%|██████▉   | 287/414 [2:18:41<1:01:42, 29.15s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 288/414 [2:19:10<1:01:09, 29.12s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 289/414 [2:19:39<1:00:37, 29.10s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 290/414 [2:20:08<1:00:06, 29.08s/it]\u001b[A\n",
            "Iteration:  70%|███████   | 291/414 [2:20:37<59:32, 29.04s/it]  \u001b[A\n",
            "Iteration:  71%|███████   | 292/414 [2:21:06<58:59, 29.02s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 293/414 [2:21:35<58:30, 29.01s/it]\u001b[A\n",
            "Iteration:  71%|███████   | 294/414 [2:22:04<58:02, 29.02s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 295/414 [2:22:33<57:31, 29.01s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 296/414 [2:23:02<56:57, 28.97s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 297/414 [2:23:31<56:23, 28.92s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 298/414 [2:24:00<55:56, 28.93s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 299/414 [2:24:28<55:23, 28.90s/it]\u001b[A\n",
            "Iteration:  72%|███████▏  | 300/414 [2:24:57<54:54, 28.90s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 301/414 [2:25:26<54:25, 28.90s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 302/414 [2:25:55<53:51, 28.85s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 303/414 [2:26:24<53:22, 28.85s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 304/414 [2:26:53<52:55, 28.87s/it]\u001b[A\n",
            "Iteration:  74%|███████▎  | 305/414 [2:27:22<52:27, 28.88s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 306/414 [2:27:50<51:56, 28.86s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 307/414 [2:28:19<51:27, 28.85s/it]\u001b[A\n",
            "Iteration:  74%|███████▍  | 308/414 [2:28:48<51:00, 28.87s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 309/414 [2:29:17<50:33, 28.89s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 310/414 [2:29:46<50:15, 29.00s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 311/414 [2:30:15<49:44, 28.98s/it]\u001b[A\n",
            "Iteration:  75%|███████▌  | 312/414 [2:30:44<49:10, 28.93s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 313/414 [2:31:13<48:38, 28.90s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 314/414 [2:31:42<48:07, 28.88s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 315/414 [2:32:10<47:36, 28.85s/it]\u001b[A\n",
            "Iteration:  76%|███████▋  | 316/414 [2:32:39<47:07, 28.85s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 317/414 [2:33:08<46:38, 28.85s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 318/414 [2:33:37<46:07, 28.83s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 319/414 [2:34:06<45:42, 28.87s/it]\u001b[A\n",
            "Iteration:  77%|███████▋  | 320/414 [2:34:35<45:18, 28.92s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 321/414 [2:35:04<44:49, 28.92s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 322/414 [2:35:33<44:20, 28.92s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 323/414 [2:36:02<43:53, 28.94s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 324/414 [2:36:31<43:24, 28.94s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 325/414 [2:37:00<42:57, 28.96s/it]\u001b[A\n",
            "Iteration:  79%|███████▊  | 326/414 [2:37:29<42:30, 28.98s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 327/414 [2:37:58<42:05, 29.02s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 328/414 [2:38:27<41:32, 28.98s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 329/414 [2:38:56<41:02, 28.97s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 330/414 [2:39:25<40:31, 28.95s/it]\u001b[A\n",
            "Iteration:  80%|███████▉  | 331/414 [2:39:54<40:05, 28.99s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 332/414 [2:40:23<39:35, 28.97s/it]\u001b[A\n",
            "Iteration:  80%|████████  | 333/414 [2:40:52<39:06, 28.97s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 334/414 [2:41:21<38:36, 28.95s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 335/414 [2:41:49<38:06, 28.94s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 336/414 [2:42:18<37:35, 28.91s/it]\u001b[A\n",
            "Iteration:  81%|████████▏ | 337/414 [2:42:47<37:06, 28.91s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 338/414 [2:43:16<36:37, 28.91s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 339/414 [2:43:45<36:13, 28.98s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 340/414 [2:44:14<35:44, 28.99s/it]\u001b[A\n",
            "Iteration:  82%|████████▏ | 341/414 [2:44:43<35:15, 28.98s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 342/414 [2:45:12<34:48, 29.00s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 343/414 [2:45:41<34:17, 28.98s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 344/414 [2:46:10<33:45, 28.93s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 345/414 [2:46:39<33:16, 28.93s/it]\u001b[A\n",
            "Iteration:  84%|████████▎ | 346/414 [2:47:08<32:47, 28.94s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 347/414 [2:47:37<32:17, 28.91s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 348/414 [2:48:06<31:47, 28.90s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 349/414 [2:48:34<31:17, 28.89s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 350/414 [2:49:03<30:47, 28.87s/it]\u001b[A\n",
            "Iteration:  85%|████████▍ | 351/414 [2:49:32<30:20, 28.89s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 352/414 [2:50:01<29:54, 28.94s/it]\u001b[A\n",
            "Iteration:  85%|████████▌ | 353/414 [2:50:30<29:25, 28.94s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 354/414 [2:51:00<29:04, 29.07s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 355/414 [2:51:29<28:33, 29.04s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 356/414 [2:51:58<28:03, 29.02s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 357/414 [2:52:27<27:33, 29.01s/it]\u001b[A\n",
            "Iteration:  86%|████████▋ | 358/414 [2:52:55<27:02, 28.97s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 359/414 [2:53:24<26:31, 28.94s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 360/414 [2:53:53<26:02, 28.94s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 361/414 [2:54:22<25:33, 28.94s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 362/414 [2:54:51<25:04, 28.93s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 363/414 [2:55:20<24:36, 28.96s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 364/414 [2:55:49<24:08, 28.97s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 365/414 [2:56:18<23:41, 29.00s/it]\u001b[A\n",
            "Iteration:  88%|████████▊ | 366/414 [2:56:47<23:13, 29.04s/it]\u001b[A\n",
            "Iteration:  89%|████████▊ | 367/414 [2:57:16<22:44, 29.03s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 368/414 [2:57:45<22:14, 29.01s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 369/414 [2:58:14<21:45, 29.01s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 370/414 [2:58:43<21:16, 29.02s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 371/414 [2:59:12<20:45, 28.96s/it]\u001b[A\n",
            "Iteration:  90%|████████▉ | 372/414 [2:59:41<20:15, 28.94s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 373/414 [3:00:10<19:47, 28.96s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 374/414 [3:00:39<19:20, 29.01s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 375/414 [3:01:08<18:49, 28.95s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 376/414 [3:01:37<18:20, 28.96s/it]\u001b[A\n",
            "Iteration:  91%|█████████ | 377/414 [3:02:06<17:51, 28.96s/it]\u001b[A\n",
            "Iteration:  91%|█████████▏| 378/414 [3:02:35<17:21, 28.94s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 379/414 [3:03:04<16:54, 28.97s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 380/414 [3:03:33<16:25, 29.00s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 381/414 [3:04:02<15:55, 28.97s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 382/414 [3:04:31<15:26, 28.97s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 383/414 [3:05:00<14:58, 28.99s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 384/414 [3:05:29<14:30, 29.01s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 385/414 [3:05:58<14:01, 29.03s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 386/414 [3:06:27<13:31, 28.97s/it]\u001b[A\n",
            "Iteration:  93%|█████████▎| 387/414 [3:06:56<13:01, 28.94s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 388/414 [3:07:25<12:33, 28.97s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 389/414 [3:07:54<12:03, 28.96s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 390/414 [3:08:22<11:34, 28.93s/it]\u001b[A\n",
            "Iteration:  94%|█████████▍| 391/414 [3:08:51<11:05, 28.92s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 392/414 [3:09:20<10:36, 28.91s/it]\u001b[A\n",
            "Iteration:  95%|█████████▍| 393/414 [3:09:49<10:07, 28.91s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 394/414 [3:10:18<09:37, 28.88s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 395/414 [3:10:47<09:09, 28.94s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 396/414 [3:11:16<08:40, 28.93s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 397/414 [3:11:45<08:11, 28.90s/it]\u001b[A\n",
            "Iteration:  96%|█████████▌| 398/414 [3:12:14<07:41, 28.87s/it]\u001b[A\n",
            "Iteration:  96%|█████████▋| 399/414 [3:12:42<07:12, 28.84s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 400/414 [3:13:11<06:43, 28.80s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 401/414 [3:13:40<06:14, 28.82s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 402/414 [3:14:09<05:45, 28.82s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 403/414 [3:14:38<05:16, 28.81s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 404/414 [3:15:06<04:48, 28.82s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 405/414 [3:15:35<04:19, 28.83s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 406/414 [3:16:04<03:51, 28.88s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 407/414 [3:16:33<03:21, 28.85s/it]\u001b[A\n",
            "Iteration:  99%|█████████▊| 408/414 [3:17:02<02:53, 28.84s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 409/414 [3:17:31<02:24, 28.89s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 410/414 [3:18:00<01:55, 28.88s/it]\u001b[A\n",
            "Iteration:  99%|█████████▉| 411/414 [3:18:28<01:26, 28.85s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 412/414 [3:18:57<00:57, 28.84s/it]\u001b[A\n",
            "Iteration: 100%|█████████▉| 413/414 [3:19:26<00:28, 28.83s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 414/414 [3:19:49<00:00, 27.19s/it]\u001b[A\n",
            "Epoch: 100%|██████████| 2/2 [6:39:47<00:00, 11995.31s/it]  \n",
            "06/14/2019 22:58:33 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "06/14/2019 22:58:33 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpo11vojo7\n",
            "06/14/2019 22:58:38 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   guid: dev-0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   tokens: [CLS] @ br ##eit ##bar ##t ##ne ##ws ok shannon , you tell the veterans in those locker rooms they have to stay there until the celebration of what they fought for is over . [SEP]\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_ids: 101 1030 7987 20175 8237 2102 2638 9333 7929 10881 1010 2017 2425 1996 8244 1999 2216 12625 4734 2027 2031 2000 2994 2045 2127 1996 7401 1997 2054 2027 4061 2005 2003 2058 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   label: 0 (id = 0)\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   guid: dev-1\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   tokens: [CLS] @ left ##y ##gle ##nn @ jared ##eke ##r @ book ##uni ##verse @ hash ##tag ##ze ##ma @ ralph ##lom ##bard ##i @ nathan ##hr ##ub ##in fine . . . because i could afford a gun if i wanted to . i could fit it into my budget . my budget ##ing is fine ? ? ? here in canada we have gun insurance and gun control ? and lots ##a p [SEP]\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_ids: 101 1030 2187 2100 9354 10695 1030 8334 23941 2099 1030 2338 19496 16070 1030 23325 15900 4371 2863 1030 6798 21297 21458 2072 1030 7150 8093 12083 2378 2986 1012 1012 1012 2138 1045 2071 8984 1037 3282 2065 1045 2359 2000 1012 1045 2071 4906 2009 2046 2026 5166 1012 2026 5166 2075 2003 2986 1029 1029 1029 2182 1999 2710 2057 2031 3282 5427 1998 3282 2491 1029 1998 7167 2050 1052 102 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   label: 0 (id = 0)\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   guid: dev-2\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   tokens: [CLS] hot mom sucks off step son in shower 8 min https : / / t . co / y ##0 ##zi ##9 ##f ##5 ##z ##6 ##j [SEP]\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_ids: 101 2980 3566 19237 2125 3357 2365 1999 6457 1022 8117 16770 1024 1013 1013 1056 1012 2522 1013 1061 2692 5831 2683 2546 2629 2480 2575 3501 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   label: 1 (id = 1)\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   guid: dev-3\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   tokens: [CLS] bro these are some cute butt plug ##s i ’ m trying to cop https : / / t . co / rs ##nx ##rf ##4 ##ht ##i [SEP]\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_ids: 101 22953 2122 2024 2070 10140 10007 13354 2015 1045 1521 1049 2667 2000 8872 16770 1024 1013 1013 1056 1012 2522 1013 12667 26807 12881 2549 11039 2072 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   label: 1 (id = 1)\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   *** Example ***\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   guid: dev-4\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   tokens: [CLS] arizona supreme court strikes down state legislation prohibiting medical marijuana use on public college c . . . https : / / t . co / f ##2 ##pt ##ck ##xy ##v ##p [SEP]\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_ids: 101 5334 4259 2457 9326 2091 2110 6094 26325 2966 16204 2224 2006 2270 2267 1039 1012 1012 1012 16770 1024 1013 1013 1056 1012 2522 1013 1042 2475 13876 3600 18037 2615 2361 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "06/14/2019 22:58:41 - INFO - __main__ -   label: 0 (id = 0)\n",
            "06/14/2019 22:58:42 - INFO - __main__ -   ***** Running evaluation *****\n",
            "06/14/2019 22:58:42 - INFO - __main__ -     Num examples = 320\n",
            "06/14/2019 22:58:42 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 100%|██████████| 40/40 [02:53<00:00,  4.30s/it]\n",
            "06/14/2019 23:01:35 - INFO - __main__ -   ***** Eval results *****\n",
            "06/14/2019 23:01:35 - INFO - __main__ -     eval_accuracy = 0.85\n",
            "06/14/2019 23:01:35 - INFO - __main__ -     eval_loss = 0.37016769740730526\n",
            "06/14/2019 23:01:35 - INFO - __main__ -     global_step = 828\n",
            "06/14/2019 23:01:35 - INFO - __main__ -     loss = 0.3244165244381785\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(8, 80)\n",
            "(8, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TkojZBHLcnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}